{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c084cb42",
   "metadata": {},
   "source": [
    "Uncomment the cell below if python modules need installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d37484",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy pandas matplotlib scikit-learn tpot pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa83a9eed3caac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49d61897",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad56fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot\n",
    "from tpot import TPOTClassifier\n",
    "from pandas_profiling import ProfileReport \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17caf117",
   "metadata": {},
   "source": [
    "Loading the dataset used for the practical, doing some basic exploration using pandas profilign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ea60a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset, X = input data y = class labels\n",
    "data = pd.read_csv(\"checkerboard.csv\")\n",
    "#print(data)\n",
    "\n",
    "profile = ProfileReport(data, title='Pandas Profiling Report')\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b61bdd",
   "metadata": {},
   "source": [
    "From the report above we may notice a few things. Attributes 3 and 4  look different from the rest. They don't show a normal distribution nor correlation to other attributes. Let's inspect them more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2307e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data['att3'], data['att4'],c=data['class'])\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(data['att1'], data['att5'],c=data['class'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d60ae5",
   "metadata": {},
   "source": [
    "This reveals the true nature of this dataset. It is just a checkerboard pattern specified by features 'att3' and 'att4'. All other features are irrelevant features initialised from a gaussian function. You may think that any machine learning algorithm may have an easy job in this dataset. Let's see about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the dataset into numerical X and y matrices that can be processed by scikit-learn\n",
    "X = data.iloc[:,:-1].values\n",
    "#X = data.iloc[:,[2,3]].values\n",
    "y = data.iloc[:,-1].values\n",
    "# Class labels are strings in this case, and have to be converted to integers\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d92df",
   "metadata": {},
   "source": [
    "In the next cell you see three possible options of ML pipeline: two individual classifiers (RandomForestClassifier and KNeighborsClassifier) and one AutoML method (TPOT) that automatically constructs its own pipeline. Comment/uncomment each block of lines to enable/disable each of the three options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e60f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest classifier and initialise the parameters for its grid search\n",
    "classifier = RandomForestClassifier()\n",
    "# Params dictionary of random forest\n",
    "param_grid = dict(max_depth=[2,3,4],n_estimators = [100, 200, 500])\n",
    "\n",
    "# Similarly for a K-nearest neighbours classifier\n",
    "#classifier = KNeighborsClassifier()\n",
    "#param_grid = dict(n_neighbors=[2,3,4,5,10])\n",
    "\n",
    "# Alternatively, TPOT is a AutoML system, that will automatically search for the best pipeline for the task\n",
    "#estimator = TPOTClassifier(generations=5, population_size=50, cv=5, random_state=42, verbosity=2,n_jobs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2c353",
   "metadata": {},
   "source": [
    "The next cell runs the 5-fold stratified cross-validation process. For RandomForest and KNN a nested cross-validation process is run to tune the methods using grid search. TPOT does its own process to tune itself within the call to estimator.fit. After a model has been trained we evaluate it using the test set by generating F1 scores, and extracting the probabilities of the predictions to (later) be able to generate a Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d6f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "preds = []\n",
    "actual_labels = []\n",
    "# Initialise the 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "for train_index,test_index in kf.split(X,y):\n",
    "\t# Generate the training and test partitions of X and Y for each iteration of CV\t\n",
    "\tX_train, X_test = X[train_index], X[test_index]\n",
    "\ty_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\t# Increasing the value of the verbose parameter will give more messags of the internal grid search process\n",
    "\t# Increasing n_jobs will tell it to use multiple cores to parallelise the computation\t\n",
    "\tgrid_search = GridSearchCV(classifier,param_grid=param_grid,cv=5,scoring=\"f1\",verbose=0,n_jobs=4)\n",
    "\tgrid_search.fit(X_train,y_train)\n",
    "\n",
    "\t# Printing the values of the parameters chosen by grid search\n",
    "\testimator = grid_search.best_estimator_\n",
    "    # Uncomment the next two lines if you are using the RandomForest classifier\n",
    "\tprint(\"Chosen max depth: {0}\".format(estimator.max_depth))\n",
    "\tprint(\"Chosen number of trees: {0}\".format(estimator.n_estimators))\n",
    "    # Uncomment the next line if using the KNN classifier\n",
    "\t#print(\"Number of neighbours: {0}\".format(estimator.n_neighbors))\n",
    "\n",
    "\t#Uncomment this line to train the TPOT module, and comment all the grid search lines above\n",
    "    #As TPOT is a AutoML system, it does it's own process of tuning rather than using grid search\n",
    "    #Comment lines 13 through 21 if you use TPOT\n",
    "\t#estimator.fit(X_train,y_train)\n",
    "\n",
    "\t# Predicting the test data with the optimised models\n",
    "\tpredictions = estimator.predict(X_test)\n",
    "\tscore = metrics.f1_score(y_test,predictions)\n",
    "\tscores.append(score)\n",
    "\n",
    "\t# Extract the probabiliites of predicting the 2nd class, which will use to generate the PR curve\n",
    "\tprobs = estimator.predict_proba(X_test)[:,1]\n",
    "\tpreds.extend(probs)\n",
    "\tactual_labels.extend(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb9d76",
   "metadata": {},
   "source": [
    "Generation of the overall performance scores and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the overall F1 score\n",
    "print(\"Average F1 score: {0}\".format(np.average(scores)))\n",
    "\n",
    "prec, recall, _ = metrics.precision_recall_curve(actual_labels, preds)\n",
    "print(\"AUPRC score: {0}\".format(metrics.auc(recall,prec)))\n",
    "# Generate the PR curve\n",
    "plt.plot(recall, prec, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44da7f1d",
   "metadata": {},
   "source": [
    "As you can see, the performance is very poor for all methods in this notebook. We need proper data pre-processing (feature selection) to be able to solve this problem, as will be seen in later lectures/practicals. Do you want to make the life of the ML pipelines easier? Create a version of X that only contains the relevant features by uncommenting '#X = data.iloc[:,[2,3]].values' in cell 4 and re-run all three pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6c406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
