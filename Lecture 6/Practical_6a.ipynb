{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HX-XYYbcYa0r"
   },
   "source": [
    "# Practical 6a Classification using SVM\n",
    "\n",
    "So far we have just been using the datasets provided with sklearn. However, we normally need to use datasets from other locations. So the first part of this practical will give an introduction on how to read in CVS files from a file or URL.\n",
    "\n",
    "Once you've done this you will need to process the data so that missing values are removed or replaced with sensible values. \n",
    "\n",
    "Then finally we can get down to the machine learning. The way you use SVM is very similar to what you did with Logistic Regression - so we don't need to give you all the details for that part. You can just look back at that.\n",
    "\n",
    "The data: We're going to use the titanic dataset. This dataset contains various features such as the sex of a passenger and the class they were travelling in. The idea is to see if we can predict if the passanger is likely to have survived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuVD44ytZh5t"
   },
   "source": [
    "## Reading in the data\n",
    "\n",
    "Pandas nicely provides you with a method to read in data from a CSV file. The file can either be on your local hard disk or at a URL.\n",
    "\n",
    "Once you've read in the data the first thing to do is to have a quick look at it - print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbBZUlzy0gNa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"http://homepages.cs.ncl.ac.uk/stephen.mcgough/data/titanic.csv\")\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyYr83VXZ4rZ"
   },
   "source": [
    "## What are the different features\n",
    "\n",
    "We can list the features using the list command\n",
    "\n",
    "The features are:\n",
    "\n",
    "| feature | description |\n",
    "|-----|------|\n",
    "| pclass | The class the passenger was travelling in |\n",
    "| survived | Whether the passenger survived or not |\n",
    "| name | Name of the passenger |\n",
    "| sex | Gender of the passenger |\n",
    "| age | Age of the passenger |\n",
    "| sibsp | Number of siblings or spouses onboard |\n",
    "| parch | Number of parents or children onboard |\n",
    "| ticket | Ticket number |\n",
    "| fare | How much they payed for the ticket |\n",
    "| cabin | Cabbin number |\n",
    "| embarked | Where they embarked from (C = Cherbourg; Q = Queenstown; S = Southampton) |\n",
    "| boat | Lifeboat they got onto |\n",
    "| body | Unknown |\n",
    "| home.dest | Where they were eventually travelling to |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgJdO3KO1D9u"
   },
   "outputs": [],
   "source": [
    "list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdpteSbKb-aG"
   },
   "source": [
    "We now need to look at each of the features and decide which ones to keep and which ones to delete. You can use the value_counts() method to see how many unique values there are. Below is the one for 'age'.\n",
    "\n",
    "## Exercise\n",
    "Try each of the different features and see how many unique values there are and how much missing data there is (recorded here with '?')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dit5xmp623xa"
   },
   "outputs": [],
   "source": [
    "data['age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWRqYvwDcj4Q"
   },
   "source": [
    "Removing features that either have unique values for all or too much missing data.\n",
    "\n",
    "We're going to remove the features 'name' and 'ticket' as almost all passengers have a unique name/ticket - so this is unlikely to be of help.\n",
    "\n",
    "We're going to remove 'cabin', 'boat', 'body' and 'home.dest' as most of these values are missing.\n",
    "\n",
    "To remove a feature we drop the appropriate column from the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVQmJ3JS1awu"
   },
   "outputs": [],
   "source": [
    "dataClean = data.drop(columns=['name', 'cabin', 'ticket', 'boat', 'body', 'home.dest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WumHbNZ2d0pp"
   },
   "source": [
    "We had two passengers who don't have a record of where they embarked from. We need to set a value for these - let's assume it is Southampton (S). We can come back to this later and try the other options to see if they give us better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgI6U-9R3_aK"
   },
   "outputs": [],
   "source": [
    "dataClean['embarked'].replace(['?'], ['S'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwsipFHweW7i"
   },
   "source": [
    "There are a number of passengers missing an age. For now let's assume that they have the median age of all the other passangers. We can again come back to this later and try other estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RCAjfd1-33-"
   },
   "outputs": [],
   "source": [
    "withoutMissingAge = dataClean.drop(dataClean[dataClean.age=='?'].index)\n",
    "medianAge = withoutMissingAge['age'].median()\n",
    "dataClean['age'].replace(['?'], [medianAge], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvGea8zFe0Cw"
   },
   "source": [
    "In the same way there are a number of passangers who don't have a fare value for their travels. We'll again assume that the median is a good estimate. We can come back later and try other estimates.\n",
    "\n",
    "## Exercise\n",
    "Using the example of replacing missing ages above replace the missing fares with the median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EBHi5fBB4py"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBhDc8kpfR4V"
   },
   "source": [
    "The data should now be clean. We should look at all of the remaining features to check that there are no more missing values.\n",
    "\n",
    "## Exercise\n",
    "Check each of the features for missing values. You can do this using values_counts()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pV9QaE6FDCQS"
   },
   "outputs": [],
   "source": [
    "dataClean['age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_6XCv_FfnX1"
   },
   "source": [
    "The feature 'sex' is a catagorical feature and needs to be converted into a One Hot Encoded set of features. First we use get_dummies() to create a One Hot Encoded version of the feature. But this looses all the other features so we need to join it back to the original data and delete the original feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWC_TXwmG1cC"
   },
   "outputs": [],
   "source": [
    "sexOHE = pd.get_dummies(dataClean['sex'], prefix='sex') # prefix is the text to start the new feature names with. The rest of the \n",
    "                                                        # feature name will be the catagorical value. So in this case we get\n",
    "                                                        # sex_female or sex_male\n",
    "dataClean = dataClean.join(sexOHE)\n",
    "dataClean = dataClean.drop(columns=['sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmYefNtogzao"
   },
   "source": [
    "We need to do the same for the other catagorical feature - embarked. \n",
    "\n",
    "## Exercise\n",
    "Produce a One Hot encoding for embarked in the same way as sex was done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJSFMe9hIniU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4913PsHhhL0g"
   },
   "source": [
    "## Producing the X and y data\n",
    "\n",
    "The X data is all of the features without the variable we want to predict and the y data is just the variable we want to predict. \n",
    "\n",
    "We can remove the variable we want to predict (survived) from the data to produce X.\n",
    "\n",
    "We can keep just the variable we want to predict using filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0e_pPSSEQSa"
   },
   "outputs": [],
   "source": [
    "X = dataClean.drop(columns=['survived'])\n",
    "y = dataClean.filter(['survived']).values.ravel() # we need a 1D array for y - hence the need for values.ravel() to reshape the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HK4KjJ8_iRZQ"
   },
   "source": [
    "## Split the data\n",
    "\n",
    "We can now split the data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0w8VRt5E1Uo"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccfCEDiWi--n"
   },
   "source": [
    "# Now the machine learning\n",
    "\n",
    "sklearn is really good in the sense that all of the machine learning processes work in prety much the same way:\n",
    "\n",
    "```\n",
    "# import the appropriate classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create an instance of that classifier\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "# Train it on our data\n",
    "logistic.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "So we just need to change sklearn.linear_model.LogisticRegression to sklearn.svm.SVC. Then change the instance you create to the correct classifier.\n",
    "\n",
    "## Exercises\n",
    "1. Create a Support Vector Classifer for the titanic data.\n",
    "2. For your classifier look at the accuracy, precision, recall, F1 and confusion matrix.\n",
    "3. Look at changing the C value and the kernel for SVC to see if you can improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqdu2kpimKwA"
   },
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVIM9oP5FFHu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
