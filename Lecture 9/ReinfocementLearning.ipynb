{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh7vBdHYD8hp"
   },
   "source": [
    "# Practical 9 Reinfocement Learning\n",
    "\n",
    "In this practical we're going to build up a simple environment, observer and agent from scratch. We'll be looking at a simple problem case which can be solved as a bandit problem.\n",
    "\n",
    "## The scenario:\n",
    "\n",
    "You have an autonomous boat that you want to use for dolphin watching. For simplicity we're going to say that you can only see dolphins if they're in front of the boat. The boat can be in one of ten locations (0-9) which form a straight line. At each point in time the boat will be at one of these locations and the dolpins will be at one of the others. We assume that the numbers run left to right, so 0 is left of 9.\n",
    "\n",
    "At each point in time the boat will face either LEFT or RIGHT. So if it's at position 3 and facing LEFT it can see dolphins at locations 0,1,2. \n",
    "\n",
    "The dolphins appear at random locations at each time step. If the boat is facing in the correct direction to see the dolpins it gets a reward of one while if it is facing in the wrong direction then the reward is zero. If the dolphins and boat are at the same location then the reward is zero as the dolphins don't surface.\n",
    "\n",
    "The idea is to produce a Q-Learning solution to work out which direction the boat should be facing in order to see the dolphins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJGhkyyjG94h"
   },
   "source": [
    "Let's set up some variables:\n",
    "\n",
    "alpha - the learning rate\n",
    "\n",
    "count - how many steps we've taken\n",
    "\n",
    "LEFT - the action face left\n",
    "\n",
    "RIGHT - the action face right\n",
    "\n",
    "action - the action taken - either LEFT or RIGHT, set to -1 initially as we've not got an action yet\n",
    "\n",
    "Q\\[][] - the Q-Learning matrix, initialised to all zeros\n",
    "\n",
    "epsilon - the probability we choose our action randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHdIDINIHxZ2"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "alpha = 0.1\n",
    "count = 0\n",
    "LEFT = 0 \n",
    "RIGHT = 1\n",
    "action = -1\n",
    "lastLocation = 5\n",
    "Q = [[0,0], [0,0], [0,0], [0,0], [0,0], [0,0], [0,0], [0,0], [0,0], [0,0]]\n",
    "epsilon = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYbI9zhnJI2I"
   },
   "source": [
    "We need to build up the code in reverse - so let's start with the code to select the action at random. This should produce either LEFT or RIGHT at random with equal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qcuG0OBJ7Zi"
   },
   "outputs": [],
   "source": [
    "def random_action():\n",
    "  # Pick the action of either RIGHT or LEFT - remember LEFT = 0, RIGHT = 1\n",
    "  act =\n",
    "  return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVPcWacitvEm"
   },
   "source": [
    "Now here's the code to select the argmax for the action. It searches the Q-Matrix for the largest value for a given state and returns the action for that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtRNV9P5uCRo"
   },
   "outputs": [],
   "source": [
    "def findMax(state):\n",
    "  # First check to see if all actions have the same expected reward:\n",
    "  if ():\n",
    "    # Both are the same so choose one at random\n",
    "    ret =\n",
    "  # Find the action with the greatest reward - call it ret\n",
    "  elif ():\n",
    "    ret =\n",
    "  else:  \n",
    "    ret = \n",
    "  return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiEslTV1KkyX"
   },
   "source": [
    "The environment:\n",
    "\n",
    "We need to define the environment. This takes the last action and produces the next state of the environment. As the environment is not affected by the last action we can ignore this. But this does need to be passed to the observer so it can decide on the reward. All we need to do is decide the next random location where the dolphins are going to appear. We pack the two values env and action into one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHQ0arQuLali"
   },
   "outputs": [],
   "source": [
    "def update_environment(action):\n",
    "  # choose location of next sighting - a value picked at random between 0 and 9\n",
    "  env =\n",
    "  # Pack these into a single variable for sending back\n",
    "  envpac = [env, action]\n",
    "  return envpac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7r-U2T_WuQxN"
   },
   "source": [
    "The observer takes in the environment data and uses the global information about where the dolphins were last seen to compute the reward. First we unpack the environment into the action and state. We can then compute the reward for the action and state combination using the lastLocation of the boat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1jVvVYELjXS"
   },
   "outputs": [],
   "source": [
    "def produce_observation(environment):\n",
    "  global lastLocation\n",
    "  sta = environment[0]\n",
    "  act = environment[1]\n",
    "  # if we're facing RIGHT and the dolphins are to our right (sta > lastLocation) then we get a reward of 1\n",
    "  if ():\n",
    "    rew =\n",
    "  # if we're facing LEFT and the dolphins are to our left (sta < lastLocation) then we get a reward of 1\n",
    "  elif (): \n",
    "    rew = \n",
    "  else:\n",
    "    # all other combinations give a reward of zero\n",
    "    rew =\n",
    "    # Move to the location of the dolphins\n",
    "  lastLocation =\n",
    "  return sta, rew "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VHiWfMcu1HH"
   },
   "source": [
    "We're now able to produce the agent. The first thing the agent will do is to update the Q-Matrix with the reward it recieves and the state the system was in. But the first time there may not be an action - so skip in this case. We then need to pick the action. We're using an epsilon-greedy policy so either we choose the action at random with probability epsilon or we find the best action given that we're in a particular state. We can do this using findMax(state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKbt6lLKLlSj"
   },
   "outputs": [],
   "source": [
    "def agent_decides_action(state, reward):\n",
    "  # always a reward so process it\n",
    "  if (action != -1): # first loop around so do nothing\n",
    "    # update the Q-Matrix using the equation for a bandit problem (see slides)\n",
    "    Q[state][action] =\n",
    "\n",
    "  # Now pick the next action\n",
    "  # Choose if we're going to do exploration or exploitation\n",
    "  # Pick a random number [0,1] and if it's less than epsilon we're doing exploration\n",
    "  if ():\n",
    "    # Exploration\n",
    "    # Pick an action at random\n",
    "    act =\n",
    "  else:\n",
    "    # Exploitation\n",
    "    # pick the action with the highest expected reward when the satate is state\n",
    "    act =\n",
    "  return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnNhAFEbH7Fl"
   },
   "source": [
    "From the lectures we take the psudocode for the main loop and implement it in python here:\n",
    "\n",
    "While (not_finished):\n",
    "\n",
    "  environment = Update_environment(action) \n",
    "\n",
    "  state, reward = Produce_observation(environment)\n",
    "\n",
    "  action = Agent_decides_action(state, reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q05T6B0II3aH"
   },
   "outputs": [],
   "source": [
    "while (count < 10000000): # place the psudo code below:\n",
    "  environment =\n",
    "  state, reward =\n",
    "  action =\n",
    "  count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CH_yBoWAxvkD"
   },
   "source": [
    "Let's take a look at the Q-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47YDQxGaNqOP",
    "outputId": "f5545c25-e97b-45eb-d8c0-c8a6a00ff8a4"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  print(Q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pF_bCqBYx0X-"
   },
   "source": [
    "In order to plot the matrix we need to swap rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mB08byFWeBqf"
   },
   "outputs": [],
   "source": [
    "Q2 = list(map(list, zip(*Q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lazQo1a7x6EE"
   },
   "source": [
    "Now let's plot the Q-Matrix. As we only have two actions we can do this. We colour LEFT actions Blue and RIGHT actions green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "5L5-z_IOW1lb",
    "outputId": "b1a57688-f9fe-4072-85c5-cf2e608d510e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = Q2\n",
    "X = np.arange(10)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(X + 0.0, data[0], color = 'b', width = 0.5)\n",
    "ax.bar(X + 0.5, data[1], color = 'g', width = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXvgAEVm1WKK"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Once you've got the code working above you can try:\n",
    "\n",
    "1. Increase the space from 10 cells (0-9) to 20 cells (0-19). How does this affect things?\n",
    "2. What happens if you vary the value of alpha?\n",
    "3. How does changing the maximum value for count do?\n",
    "4. How does changing epsilon affect the results?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
